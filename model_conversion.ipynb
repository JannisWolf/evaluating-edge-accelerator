{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Kopie von Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPaX2UVOOpZsUviRUXjznLG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JannisWolf/evaluating-edge-accelerator/blob/main/model_conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2-LlqGYgA_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyHSfCJyaaHR",
        "outputId": "ab834fcb-d6de-4390-f7d7-fe7dcba5cd8c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H7Ncg-lAawm1",
        "outputId": "978ee116-3890-4745-edd3-1caf670c6b2f"
      },
      "source": [
        "!ls '/content/gdrive/My Drive/data/autoencoder'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "model.onnx  net-state-3072-scaled2-standard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWOxrTOJbzDp"
      },
      "source": [
        "inputSize = 3*1024"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejv_G7k9aOCS"
      },
      "source": [
        "# auto encoder class\n",
        "\n",
        "class GrindNet(nn.Module):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    sizes = [inputSize, 256]\n",
        "    self.layers = nn.ModuleList()\n",
        "\n",
        "    for size1, size2 in zip(sizes, sizes[1:]):\n",
        "      self.layers.append(nn.Linear(in_features=size1, out_features=size2))\n",
        "    for size1, size2 in zip(reversed(sizes), list(reversed(sizes))[1:]):\n",
        "      self.layers.append(nn.Linear(in_features=size1, out_features=size2))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "      x = functional.leaky_relu(x)\n",
        "    return x"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmpkfcDtaOPO"
      },
      "source": [
        "model = GrindNet()\n",
        "path = F\"/content/gdrive/My Drive/data/autoencoder/\"\n",
        "model_name = \"net-state-3072-scaled2-standard\""
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEe0HcX-aObz",
        "outputId": "5987a4d6-35da-47c4-9186-1766e7053843"
      },
      "source": [
        "model.load_state_dict(torch.load(path+model_name))\n",
        "model.eval()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GrindNet(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=3072, out_features=256, bias=True)\n",
              "    (1): Linear(in_features=256, out_features=3072, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MgGd5N5aOnh"
      },
      "source": [
        "with torch.no_grad():\n",
        "     torch.onnx.export(model,\n",
        "     torch.randn(*(inputSize,)),\n",
        "     path + \"/model.onnx\",\n",
        "     export_params=True,\n",
        "     opset_version=10,\n",
        "     do_constant_folding=True,\n",
        "     input_names=['input'],\n",
        "     output_names=['output'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpSzWqzoaPQu"
      },
      "source": [
        "!pip install onnx==1.8.1 onnx2keras pytorch2keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG8ha8jtaPDb"
      },
      "source": [
        "import onnx\n",
        "from onnx2keras import onnx_to_keras\n",
        "from pytorch2keras.converter import pytorch_to_keras\n",
        "from torch.autograd import Variable\n",
        "import tensorflow as tf"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqFxAHv1c1OS",
        "outputId": "18ec0682-5d5f-4736-dcaf-a0250525e72d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "input_np = np.random.uniform(0, 1, (3072)).astype('float32')\n",
        "a = Variable(torch.FloatTensor(input_np))\n",
        "print(input_np)\n",
        "print(a)\n",
        "\n",
        "# a = torch.rand(3072)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.11086931 0.87515277 0.27407765 ... 0.2219126  0.615592   0.84407085]\n",
            "tensor([0.1109, 0.8752, 0.2741,  ..., 0.2219, 0.6156, 0.8441])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaoxjFfkcgsc"
      },
      "source": [
        "k_model = pytorch_to_keras(model, a,input_shapes=[(3072,)], verbose=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOf24-p4evGg",
        "outputId": "6643981e-c765-45bf-de2f-040b7117969c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "k_model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_0 (InputLayer)            [(None, 3072)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "7_const1 (Lambda)               (256,)               0           input_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "6 (Dense)                       (None, 256)          786432      input_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "7 (Lambda)                      (None, 256)          0           7_const1[0][0]                   \n",
            "                                                                 6[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "8 (LeakyReLU)                   (None, 256)          0           7[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "11_const1 (Lambda)              (3072,)              0           input_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "10 (Dense)                      (None, 3072)         786432      8[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "11 (Lambda)                     (None, 3072)         0           11_const1[0][0]                  \n",
            "                                                                 10[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "output_0 (LeakyReLU)            (None, 3072)         0           11[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 1,572,864\n",
            "Trainable params: 1,572,864\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDzAKiwTe_YQ",
        "outputId": "e9dc31fc-66e2-4b89-c071-fd92b2518c85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(model)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GrindNet(\n",
            "  (layers): ModuleList(\n",
            "    (0): Linear(in_features=3072, out_features=256, bias=True)\n",
            "    (1): Linear(in_features=256, out_features=3072, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj9ONeeXcg5N",
        "outputId": "99e526ba-313b-4c8c-ea29-d956c18540d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# convert pytorch tensor to tf tensor\n",
        "a_tf = tf.convert_to_tensor([input_np]) \n",
        "\n",
        "# run inference on the model\n",
        "p = model(a)\n",
        "k = k_model.predict(a_tf)\n",
        "\n",
        "# print results\n",
        "print(\"Input Pytorch {}\".format(a))\n",
        "print(\"Input Tensorflow tensor({})\".format(a_tf[0]))\n",
        "print(\"Output Pytorch {}\".format(p))\n",
        "print(\"Output Tensorflow tensor({})\".format(k[0]))\n",
        "\n",
        "# equal function as the precision differs\n",
        "def equal(l1, l2, p=False):\n",
        "  '''\n",
        "  Checks to which precision it is equal\n",
        "  '''\n",
        "  diff = abs(l1 - l2)\n",
        "  max_diff = np.max(diff)\n",
        "  if p:\n",
        "    print(\"Maximum difference is {}\".format(max_diff))\n",
        "  for i in range(10):\n",
        "    prec = 10**-i\n",
        "    if max_diff > prec:\n",
        "      p = i\n",
        "      break\n",
        "  return \"Equal until 10^-{}.\".format(p)\n",
        "\n",
        "# check if the values are the same\n",
        "print(equal(k,p.detach().numpy()))"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Pytorch tensor([0.1109, 0.8752, 0.2741,  ..., 0.2219, 0.6156, 0.8441])\n",
            "Input Tensorflow tensor([0.11086931 0.87515277 0.27407765 ... 0.2219126  0.615592   0.84407085])\n",
            "Output Pytorch tensor([ 2.6314,  2.3040,  3.3686,  ..., -0.0494,  0.0215,  2.4654],\n",
            "       grad_fn=<LeakyReluBackward0>)\n",
            "Output Tensorflow tensor([ 2.6314406   2.304026    3.368564   ... -0.04941374  0.02154568\n",
            "  2.4654353 ])\n",
            "Equal until 10^-5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dO5aNuXgd3e",
        "outputId": "2722a8c6-8b08-4f35-dcec-ac3604ebaa7e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# prerequisites of the tensorflow lite inference\n",
        "SAVED_MODEL_PATH = '/content/gdrive/My Drive/data/saved_models/test_variable'\n",
        "TFLITE_FILE_PATH = '/content/gdrive/My Drive/data/test_variable.tflite'\n",
        "\n",
        "tf.saved_model.save(\n",
        "    k_model, SAVED_MODEL_PATH)\n",
        "\n",
        "#converter = tf.lite.TFLiteConverter.from_keras_model(k_model)\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(TFLITE_FILE_PATH, 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"model_4/7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"model_4/6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"model_4/11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"model_4/10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(256,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(3072,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(3072,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(256,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(256,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(256,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(3072,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(3072,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 3072), dtype=float32)\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/data/saved_models/test_variable/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/data/saved_models/test_variable/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_VF3oohdwRu",
        "outputId": "4f22e2b4-45ab-4317-a121-a6c9dfba32dd"
      },
      "source": [
        "# Load the TFLite model in TFLite Interpreter\n",
        "interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], [input_np])\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.631441    2.3040261   3.3685653  ... -0.04941375  0.02154616\n",
            "   2.4654405 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4FTd7gJdwe5",
        "outputId": "28951834-42a0-4253-eac8-7182c11f8c70"
      },
      "source": [
        "print(\"Pytorch vs. Tensorflow\")\n",
        "print(equal(k,p.detach().numpy(), p=True) + '\\n')\n",
        "print(\"Pytorch vs. Tensorflow lite\")\n",
        "print(equal(output_data,p.detach().numpy(), p=True) + '\\n')\n",
        "print(\"Tensorflow vs Tensorflow lite\")\n",
        "print(equal(k, output_data, p=True) + '\\n')"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch vs. Tensorflow\n",
            "Maximum difference is 1.1682510375976562e-05\n",
            "Equal until 10^-5.\n",
            "\n",
            "Pytorch vs. Tensorflow lite\n",
            "Maximum difference is 1.2159347534179688e-05\n",
            "Equal until 10^-5.\n",
            "\n",
            "Tensorflow vs Tensorflow lite\n",
            "Maximum difference is 1.1444091796875e-05\n",
            "Equal until 10^-5.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLu0S2a7dwxj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr0FZ_8Ndw9K"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ9YRwUflttR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}