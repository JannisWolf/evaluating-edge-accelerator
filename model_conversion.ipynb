{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Kopie von Kopie von Kopie von Untitled2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPelmtHfWmYwUFXA1uJLqhU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JannisWolf/evaluating-edge-accelerators/blob/JannisWolf-models/model_conversion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKHhLxYX0DsR"
      },
      "source": [
        "In this notebook the conversion from a pytorch model to keras, tf lite and onnx happens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2-LlqGYgA_"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as functional\n",
        "import torch.optim as optim\n",
        "\n",
        "import numpy as np"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyHSfCJyaaHR",
        "outputId": "e6c9a598-8ef3-4d7a-d47f-672f139505ac"
      },
      "source": [
        "# remove if not using colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TWOxrTOJbzDp"
      },
      "source": [
        "inputSize = 3*1024"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejv_G7k9aOCS"
      },
      "source": [
        "# auto encoder class\n",
        "# if relu activation is needed change here in the forward method leaky_relu to relu\n",
        "\n",
        "class GrindNet(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        sizes = [inputSize, 256]\n",
        "        self.layers = nn.ModuleList()\n",
        "\n",
        "        for size1, size2 in zip(sizes, sizes[1:]):\n",
        "            self.layers.append(nn.Linear(in_features=size1, out_features=size2))\n",
        "        for size1, size2 in zip(reversed(sizes), list(reversed(sizes))[1:]):\n",
        "            self.layers.append(nn.Linear(in_features=size1, out_features=size2))\n",
        "  \n",
        "    def forward(self, x):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "            x = functional.leaky_relu(x)\n",
        "        return x"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmpkfcDtaOPO"
      },
      "source": [
        "model = GrindNet()\n",
        "path = F\"/content/gdrive/My Drive/data/autoencoder/\"\n",
        "model_name = \"net-state-3072-scaled2-standard\""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEe0HcX-aObz",
        "outputId": "8ccc5f74-5814-49d2-fe50-489ccd04ca59"
      },
      "source": [
        "# load pytorch model\n",
        "model.load_state_dict(torch.load(path+model_name))\n",
        "model.eval()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GrindNet(\n",
              "  (layers): ModuleList(\n",
              "    (0): Linear(in_features=3072, out_features=256, bias=True)\n",
              "    (1): Linear(in_features=256, out_features=3072, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MgGd5N5aOnh"
      },
      "source": [
        "# save back to onnx\n",
        "x=torch.randn(*(inputSize,))\n",
        "with torch.no_grad():\n",
        "     torch.onnx.export(model,\n",
        "     x,\n",
        "     path + \"models/model.onnx\",\n",
        "     export_params=True,\n",
        "     opset_version=10,\n",
        "     do_constant_folding=True,\n",
        "     input_names=['input'],\n",
        "     output_names=['output'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpSzWqzoaPQu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d0f9ce9-c7f3-4248-bdc0-fd0302b3f211"
      },
      "source": [
        "# easy way to convert to keras/tensorflow was using pytorch2keras which needed exaclty the right onnx version (optimizer class in newer onnx is deprecated)\n",
        "!pip install onnx==1.8.1 onnx2keras pytorch2keras"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnx==1.8.1 in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: onnx2keras in /usr/local/lib/python3.7/dist-packages (0.0.24)\n",
            "Requirement already satisfied: pytorch2keras in /usr/local/lib/python3.7/dist-packages (0.2.4)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnx==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from onnx==1.8.1) (1.15.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnx==1.8.1) (3.17.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (from onnx2keras) (2.5.0)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (from pytorch2keras) (2.4.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from pytorch2keras) (1.9.0+cu102)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from pytorch2keras) (0.10.0+cu102)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras->pytorch2keras) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras->pytorch2keras) (3.1.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras->pytorch2keras) (3.13)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras->pytorch2keras) (1.5.2)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (3.3.0)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (0.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (0.12.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (0.36.2)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (1.34.1)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (1.12)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (2.5.0)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (1.1.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow->onnx2keras) (1.12.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->onnx2keras) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->onnx2keras) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->onnx2keras) (57.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->onnx2keras) (0.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->onnx2keras) (2.23.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->onnx2keras) (1.32.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->onnx2keras) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow->onnx2keras) (1.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->onnx2keras) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->onnx2keras) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->onnx2keras) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->onnx2keras) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow->onnx2keras) (4.6.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow->onnx2keras) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->onnx2keras) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->onnx2keras) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->onnx2keras) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.5->tensorflow->onnx2keras) (2.10)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow->onnx2keras) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.5->tensorflow->onnx2keras) (3.5.0)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->pytorch2keras) (7.1.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mG8ha8jtaPDb"
      },
      "source": [
        "import onnx\n",
        "from onnx2keras import onnx_to_keras\n",
        "from pytorch2keras.converter import pytorch_to_keras\n",
        "from torch.autograd import Variable\n",
        "import tensorflow as tf"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqFxAHv1c1OS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2b039d4-27de-4d83-b0b3-f422e7329658"
      },
      "source": [
        "np.random.seed(42)\n",
        "input_np = np.random.uniform(0, 1, (3072)).astype('float32')\n",
        "a = Variable(torch.FloatTensor(input_np))\n",
        "print(a)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.3745, 0.9507, 0.7320,  ..., 0.6760, 0.7066, 0.6100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaoxjFfkcgsc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "224fdbad-6602-48d9-9378-fb5acd7b24aa"
      },
      "source": [
        "# convert from pytorch to keras model (needs a typical input -> a)\n",
        "k_model = pytorch_to_keras(model, a, input_shapes=[(3072,)], verbose=True)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch2keras:Converter is called.\n",
            "WARNING:pytorch2keras:Custom shapes isn't supported now.\n",
            "DEBUG:pytorch2keras:Input_names:\n",
            "DEBUG:pytorch2keras:['input_0']\n",
            "DEBUG:pytorch2keras:Output_names:\n",
            "DEBUG:pytorch2keras:['output_0']\n",
            "INFO:onnx2keras:Converter is called.\n",
            "DEBUG:onnx2keras:List input shapes:\n",
            "DEBUG:onnx2keras:[(3072,)]\n",
            "DEBUG:onnx2keras:List inputs:\n",
            "DEBUG:onnx2keras:Input 0 -> input_0.\n",
            "DEBUG:onnx2keras:List outputs:\n",
            "DEBUG:onnx2keras:Output 0 -> output_0.\n",
            "DEBUG:onnx2keras:Gathering weights to dictionary.\n",
            "DEBUG:onnx2keras:Found weight layers.0.weight with shape (256, 3072).\n",
            "DEBUG:onnx2keras:Found weight layers.0.bias with shape (256,).\n",
            "DEBUG:onnx2keras:Found weight layers.1.weight with shape (3072, 256).\n",
            "DEBUG:onnx2keras:Found weight layers.1.bias with shape (3072,).\n",
            "DEBUG:onnx2keras:Found input input_0 with shape (3072,)\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Transpose\n",
            "DEBUG:onnx2keras:node_name: 5\n",
            "DEBUG:onnx2keras:node_params: {'perm': [1, 0], 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name layers.0.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "WARNING:onnx2keras:transpose:Can't permute batch dimension. Result may be wrong.\n",
            "WARNING:onnx2keras:transpose:Transposing numpy array.\n",
            "DEBUG:onnx2keras:Output TF Layer -> [[-0.11485884  0.20277007  0.20645198 ... -0.11315982  0.037769\n",
            "  -0.03067089]\n",
            " [-0.03531049  0.18202555  0.32490447 ... -0.11240902  0.0483281\n",
            "   0.01573502]\n",
            " [-0.05169361  0.21194446  0.31460235 ... -0.27123502  0.15433949\n",
            "   0.01340038]\n",
            " ...\n",
            " [ 0.02156981  0.66309786  0.26089156 ...  0.303598   -0.02058391\n",
            "   0.4441051 ]\n",
            " [ 0.02215991  0.47596595  0.1668988  ...  0.20170341  0.03718566\n",
            "   0.19712137]\n",
            " [ 0.03618103  0.37300557  0.12109304 ... -0.01122106  0.02621819\n",
            "   0.00824091]]\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: MatMul\n",
            "DEBUG:onnx2keras:node_name: 6\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name input_0).\n",
            "DEBUG:onnx2keras:Check input 1 (name 5).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM without bias.\n",
            "DEBUG:onnx2keras:gemm:Input units 3072, output units 256.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "graph(%input_0 : Float(3072, strides=[1], requires_grad=0, device=cpu),\n",
            "      %layers.0.weight : Float(256, 3072, strides=[3072, 1], requires_grad=1, device=cpu),\n",
            "      %layers.0.bias : Float(256, strides=[1], requires_grad=1, device=cpu),\n",
            "      %layers.1.weight : Float(3072, 256, strides=[256, 1], requires_grad=1, device=cpu),\n",
            "      %layers.1.bias : Float(3072, strides=[1], requires_grad=1, device=cpu)):\n",
            "  %5 : Float(3072, 256, strides=[256, 1], device=cpu) = onnx::Transpose[perm=[1, 0]](%layers.0.weight)\n",
            "  %6 : Float(256, strides=[1], device=cpu) = onnx::MatMul(%input_0, %5)\n",
            "  %7 : Float(256, strides=[1], requires_grad=1, device=cpu) = onnx::Add(%layers.0.bias, %6) # /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1847:0\n",
            "  %8 : Float(256, strides=[1], requires_grad=1, device=cpu) = onnx::LeakyRelu[alpha=0.01](%7) # /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1474:0\n",
            "  %9 : Float(256, 3072, strides=[3072, 1], device=cpu) = onnx::Transpose[perm=[1, 0]](%layers.1.weight)\n",
            "  %10 : Float(3072, strides=[1], device=cpu) = onnx::MatMul(%8, %9)\n",
            "  %11 : Float(3072, strides=[1], requires_grad=1, device=cpu) = onnx::Add(%layers.1.bias, %10) # /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1847:0\n",
            "  %output_0 : Float(3072, strides=[1], requires_grad=1, device=cpu) = onnx::LeakyRelu[alpha=0.01](%11) # /usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1474:0\n",
            "  return (%output_0)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='6/MatMul:0', description=\"created by layer '6'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Add\n",
            "DEBUG:onnx2keras:node_name: 7\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name layers.0.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 1 (name 6).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:add:Convert inputs to Keras/TF layers if needed.\n",
            "WARNING:onnx2keras:add:Failed to use keras.layers.Add. Fallback to TF lambda.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='7/Add:0', description=\"created by layer '7'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: LeakyRelu\n",
            "DEBUG:onnx2keras:node_name: 8\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 0.009999999776482582, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 7).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 256), dtype=tf.float32, name=None), name='8/LeakyRelu:0', description=\"created by layer '8'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Transpose\n",
            "DEBUG:onnx2keras:node_name: 9\n",
            "DEBUG:onnx2keras:node_params: {'perm': [1, 0], 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name layers.1.weight).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "WARNING:onnx2keras:transpose:Can't permute batch dimension. Result may be wrong.\n",
            "WARNING:onnx2keras:transpose:Transposing numpy array.\n",
            "DEBUG:onnx2keras:Output TF Layer -> [[-0.04871261 -0.05055617 -0.04172203 ...  0.2671511   0.22731899\n",
            "   0.30889818]\n",
            " [ 0.01340079  0.00582984  0.00904713 ... -0.01492004 -0.04817236\n",
            "  -0.02423363]\n",
            " [ 0.00122552  0.0040424  -0.00263909 ...  0.06556262  0.04742686\n",
            "   0.07182909]\n",
            " ...\n",
            " [-0.02347532 -0.02875859 -0.06264582 ...  0.08731677  0.06477282\n",
            "   0.08458523]\n",
            " [-0.01122399 -0.01339267 -0.04879461 ...  0.00968153  0.04435373\n",
            "   0.03026479]\n",
            " [-0.00240653 -0.00355    -0.01066378 ...  0.03610542 -0.02556777\n",
            "  -0.0284941 ]]\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: MatMul\n",
            "DEBUG:onnx2keras:node_name: 10\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 8).\n",
            "DEBUG:onnx2keras:Check input 1 (name 9).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:gemm:Convert GEMM without bias.\n",
            "DEBUG:onnx2keras:gemm:Input units 256, output units 3072.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 3072), dtype=tf.float32, name=None), name='10/MatMul:0', description=\"created by layer '10'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: Add\n",
            "DEBUG:onnx2keras:node_name: 11\n",
            "DEBUG:onnx2keras:node_params: {'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name layers.1.bias).\n",
            "DEBUG:onnx2keras:The input not found in layers / model inputs.\n",
            "DEBUG:onnx2keras:Found in weights, add as a numpy constant.\n",
            "DEBUG:onnx2keras:Check input 1 (name 10).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:add:Convert inputs to Keras/TF layers if needed.\n",
            "WARNING:onnx2keras:add:Failed to use keras.layers.Add. Fallback to TF lambda.\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 3072), dtype=tf.float32, name=None), name='11/Add:0', description=\"created by layer '11'\")\n",
            "DEBUG:onnx2keras:######\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Converting ONNX operation\n",
            "DEBUG:onnx2keras:type: LeakyRelu\n",
            "DEBUG:onnx2keras:node_name: output_0\n",
            "DEBUG:onnx2keras:node_params: {'alpha': 0.009999999776482582, 'change_ordering': False, 'name_policy': None}\n",
            "DEBUG:onnx2keras:...\n",
            "DEBUG:onnx2keras:Check if all inputs are available:\n",
            "DEBUG:onnx2keras:Check input 0 (name 11).\n",
            "DEBUG:onnx2keras:... found all, continue\n",
            "DEBUG:onnx2keras:Output TF Layer -> KerasTensor(type_spec=TensorSpec(shape=(None, 3072), dtype=tf.float32, name=None), name='output_0/LeakyRelu:0', description=\"created by layer 'output_0'\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Placeholder:0\", shape=(256,), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"Placeholder:0\", shape=(3072,), dtype=float32) Tensor(\"Placeholder_1:0\", shape=(None, 3072), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOf24-p4evGg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "adfdf203-cc16-4bd7-82b8-8fe74c0fed91"
      },
      "source": [
        "# controll if conversion was succesful (Lambda layer with no trainable parameter are apperently only const layer)\n",
        "k_model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_0 (InputLayer)            [(None, 3072)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "7_const1 (Lambda)               (256,)               0           input_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "6 (Dense)                       (None, 256)          786432      input_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "7 (Lambda)                      (None, 256)          0           7_const1[0][0]                   \n",
            "                                                                 6[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "8 (LeakyReLU)                   (None, 256)          0           7[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "11_const1 (Lambda)              (3072,)              0           input_0[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "10 (Dense)                      (None, 3072)         786432      8[0][0]                          \n",
            "__________________________________________________________________________________________________\n",
            "11 (Lambda)                     (None, 3072)         0           11_const1[0][0]                  \n",
            "                                                                 10[0][0]                         \n",
            "__________________________________________________________________________________________________\n",
            "output_0 (LeakyReLU)            (None, 3072)         0           11[0][0]                         \n",
            "==================================================================================================\n",
            "Total params: 1,572,864\n",
            "Trainable params: 1,572,864\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Z_XWEqeyMix",
        "outputId": "a1fe3e68-fe14-4dc0-9a29-09d363580336"
      },
      "source": [
        "# save tensorflow model\n",
        "k_model.save(path + \"models/keras_model\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"model/7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"model/6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"model/11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"model/10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(256,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(3072,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(3072,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(256,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(256,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(256,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(3072,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(3072,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 3072), dtype=float32)\n",
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/data/autoencoder/models/keras_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/data/autoencoder/models/keras_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj9ONeeXcg5N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58587ff4-0523-40b0-a747-c347a5be9106"
      },
      "source": [
        "# just some checks if pytorch and tensorflow output matches and how precisely (can be skipped)\n",
        "\n",
        "# convert pytorch tensor to tf tensor\n",
        "a_tf = tf.convert_to_tensor([input_np]) \n",
        "\n",
        "# run inference on the model\n",
        "p = model(a)\n",
        "k = k_model.predict(a_tf)\n",
        "\n",
        "# print results\n",
        "print(\"Input Pytorch {}\".format(a))\n",
        "print(\"Input Tensorflow tensor({})\".format(a_tf[0]))\n",
        "print(\"Output Pytorch {}\".format(p))\n",
        "print(\"Output Tensorflow tensor({})\".format(k[0]))\n",
        "\n",
        "# equal function as the precision differs\n",
        "def equal(l1, l2, p=False):\n",
        "  '''\n",
        "  Checks to which precision it is equal\n",
        "  '''\n",
        "  diff = abs(l1 - l2)\n",
        "  max_diff = np.max(diff)\n",
        "  if p:\n",
        "    print(\"Maximum difference is {}\".format(max_diff))\n",
        "  for i in range(10):\n",
        "    prec = 10**-i\n",
        "    if max_diff > prec:\n",
        "      p = i\n",
        "      break\n",
        "  return \"Equal until 10^-{}.\".format(p)\n",
        "\n",
        "# check if the values are the same\n",
        "print(equal(k,p.detach().numpy()))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"model/7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"model/6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"model/11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"model/10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Input Pytorch tensor([0.3745, 0.9507, 0.7320,  ..., 0.6760, 0.7066, 0.6100])\n",
            "Input Tensorflow tensor([0.37454012 0.9507143  0.7319939  ... 0.6760263  0.7066299  0.6100074 ])\n",
            "Output Pytorch tensor([ 2.2478,  2.2217,  3.5181,  ..., -0.0254,  3.6830,  5.7795],\n",
            "       grad_fn=<LeakyReluBackward0>)\n",
            "Output Tensorflow tensor([ 2.2477582   2.2216947   3.5181472  ... -0.02544572  3.6830046\n",
            "  5.779535  ])\n",
            "Equal until 10^-5.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dO5aNuXgd3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cdfbfaf-bcf2-4d13-c019-c0d462b1c047"
      },
      "source": [
        "# prerequisites of the tensorflow lite inference\n",
        "SAVED_MODEL_PATH = path + 'models/model'\n",
        "TFLITE_FILE_PATH = path + 'models/model.tflite'\n",
        "\n",
        "tf.saved_model.save(\n",
        "    k_model, SAVED_MODEL_PATH)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(SAVED_MODEL_PATH)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(TFLITE_FILE_PATH, 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"model/7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"model/6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"model/11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"model/10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(256,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(3072,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(3072,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs:0\", shape=(256,), dtype=float32) Tensor(\"inputs_1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"7_const1/Const:0\", shape=(256,), dtype=float32) Tensor(\"6/MatMul:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"11_const1/Const:0\", shape=(3072,), dtype=float32) Tensor(\"10/MatMul:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(256,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(256,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 256), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(3072,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 3072), dtype=float32)\n",
            "Tensor(\"inputs/0:0\", shape=(3072,), dtype=float32) Tensor(\"inputs/1:0\", shape=(None, 3072), dtype=float32)\n",
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:FOR KERAS USERS: The object that you are saving contains one or more Keras models or layers. If you are loading the SavedModel with `tf.keras.models.load_model`, continue reading (otherwise, you may ignore the following instructions). Please change your code to save with `tf.keras.models.save_model` or `model.save`, and confirm that the file \"keras.metadata\" exists in the export directory. In the future, Keras will only load the SavedModels that have this file. In other words, `tf.saved_model.save` will no longer write SavedModels that can be recovered as Keras models (this will apply in TF 2.5).\n",
            "\n",
            "FOR DEVS: If you are overwriting _tracking_metadata in your class, this property has been used to save metadata in the SavedModel. The metadta field will be deprecated soon, so please move the metadata to a different file.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/data/autoencoder/models/model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: /content/gdrive/My Drive/data/autoencoder/models/model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_VF3oohdwRu",
        "outputId": "a3ed83f6-607b-449f-e2e0-b4cb19d5fe52"
      },
      "source": [
        "# Load the TFLite model in TFLite Interpreter\n",
        "interpreter = tf.lite.Interpreter(TFLITE_FILE_PATH)\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "interpreter.set_tensor(input_details[0]['index'], [input_np])\n",
        "\n",
        "interpreter.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
        "print(output_data)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 2.2477562   2.2216935   3.518146   ... -0.02544577  3.682998\n",
            "   5.7795305 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4FTd7gJdwe5",
        "outputId": "cb48223f-b015-4b23-b2c0-150e04cc15b2"
      },
      "source": [
        "# test output difference of pytorch vs tf vs tflite\n",
        "\n",
        "print(\"Pytorch vs. Tensorflow\")\n",
        "print(equal(k,p.detach().numpy(), p=True) + '\\n')\n",
        "print(\"Pytorch vs. Tensorflow lite\")\n",
        "print(equal(output_data,p.detach().numpy(), p=True) + '\\n')\n",
        "print(\"Tensorflow vs Tensorflow lite\")\n",
        "print(equal(k, output_data, p=True) + '\\n')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch vs. Tensorflow\n",
            "Maximum difference is 1.5497207641601562e-05\n",
            "Equal until 10^-5.\n",
            "\n",
            "Pytorch vs. Tensorflow lite\n",
            "Maximum difference is 9.5367431640625e-06\n",
            "Equal until 10^-6.\n",
            "\n",
            "Tensorflow vs Tensorflow lite\n",
            "Maximum difference is 1.71661376953125e-05\n",
            "Equal until 10^-5.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLu0S2a7dwxj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ec06c3-0e9a-4d89-d615-d4546aa68d04"
      },
      "source": [
        "# test data for quantization\n",
        "path2data = path + 'testseqs.npz'\n",
        "\n",
        "def loadTensor(fileName):\n",
        "  if fileName.endswith(\"npz\"):\n",
        "    return np.load(fileName, encoding=\"bytes\", allow_pickle=True)[\"arr_0\"]\n",
        "\n",
        "def flattenData(data):\n",
        "  data = list(data)\n",
        "  for i, seq in enumerate(data):\n",
        "    seq = seq[1:]\n",
        "    data[i] = seq.flatten()\n",
        "  data = np.array(data)\n",
        "  return data\n",
        "\n",
        "def preprocess(data):\n",
        "  for seq in data:\n",
        "    maximum = np.max(seq)\n",
        "    minimum = np.min(seq)\n",
        "    seq[:] -= minimum\n",
        "    seq[:] /= maximum\n",
        "  return data\n",
        "\n",
        "testData = loadTensor(path2data)\n",
        "testData = flattenData(testData)\n",
        "testData = preprocess(testData)\n",
        "testData = testData.astype(np.float32)\n",
        "testData"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[7.02596903e-02, 7.88275078e-02, 6.28098249e-02, ...,\n",
              "        8.46290410e-01, 7.18571186e-01, 7.89748132e-01],\n",
              "       [1.19231574e-01, 1.10167868e-01, 1.11724623e-01, ...,\n",
              "        1.18165521e-03, 5.64435381e-04, 0.00000000e+00],\n",
              "       [6.11173287e-02, 5.88974915e-02, 6.58349395e-02, ...,\n",
              "        8.42723668e-01, 8.39252174e-01, 6.73859239e-01],\n",
              "       ...,\n",
              "       [6.24613203e-02, 6.42610490e-02, 6.10087290e-02, ...,\n",
              "        8.42233777e-01, 8.37665260e-01, 6.41479552e-01],\n",
              "       [1.16295986e-01, 1.08052090e-01, 1.19746648e-01, ...,\n",
              "        1.15959579e-03, 1.19444542e-02, 0.00000000e+00],\n",
              "       [1.02119006e-01, 1.11704506e-01, 9.95167419e-02, ...,\n",
              "        0.00000000e+00, 2.75775641e-02, 1.08543202e-01]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rr0FZ_8Ndw9K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9fdcac86-21b2-4bcb-a356-6031a727e370"
      },
      "source": [
        "# performs also the same on the real data..\n",
        "print(model(torch.tensor(testData)))\n",
        "print(k_model.predict(testData))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 6.6296e-02,  7.4708e-02,  7.6669e-02,  ...,  8.3556e-01,\n",
            "          8.0965e-01,  7.8180e-01],\n",
            "        [ 1.0906e-01,  1.1374e-01,  1.2028e-01,  ..., -1.1100e-03,\n",
            "         -1.4907e-03, -3.5280e-03],\n",
            "        [ 5.7227e-02,  6.5812e-02,  6.5342e-02,  ...,  8.3165e-01,\n",
            "          8.0111e-01,  7.6677e-01],\n",
            "        ...,\n",
            "        [ 5.4487e-02,  6.3310e-02,  6.4840e-02,  ...,  8.3590e-01,\n",
            "          7.9097e-01,  7.4725e-01],\n",
            "        [ 1.0580e-01,  1.1005e-01,  1.1634e-01,  ..., -1.2727e-03,\n",
            "         -1.7938e-03, -4.0187e-03],\n",
            "        [ 9.5722e-02,  1.0589e-01,  1.1079e-01,  ...,  2.2146e-01,\n",
            "         -3.4933e-05, -3.8865e-03]], grad_fn=<LeakyReluBackward0>)\n",
            "[[ 6.62963837e-02  7.47077242e-02  7.66690522e-02 ...  8.35561275e-01\n",
            "   8.09653521e-01  7.81801105e-01]\n",
            " [ 1.09055340e-01  1.13743551e-01  1.20280266e-01 ... -1.11002685e-03\n",
            "  -1.49066595e-03 -3.52803431e-03]\n",
            " [ 5.72268665e-02  6.58124462e-02  6.53415322e-02 ...  8.31651568e-01\n",
            "   8.01107287e-01  7.66771555e-01]\n",
            " ...\n",
            " [ 5.44868559e-02  6.33098856e-02  6.48399591e-02 ...  8.35904956e-01\n",
            "   7.90972114e-01  7.47254968e-01]\n",
            " [ 1.05804533e-01  1.10045172e-01  1.16337731e-01 ... -1.27271411e-03\n",
            "  -1.79375196e-03 -4.01871977e-03]\n",
            " [ 9.57223326e-02  1.05885379e-01  1.10788360e-01 ...  2.21455485e-01\n",
            "  -3.49369620e-05 -3.88655392e-03]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fNmSzKAfgFX",
        "outputId": "d62e8cde-4bb7-43e1-ea06-d97a4adc8091"
      },
      "source": [
        "# helper function for reconstruction loss\n",
        "def rmse(a,b):\n",
        "  tmp = (a-b)**2\n",
        "  tmp = np.sum(tmp, axis=1)\n",
        "  return np.sqrt(tmp)/np.shape(a)[1]\n",
        "\n",
        "print(rmse(testData, k_model.predict(testData)).mean())\n",
        "print(rmse(model(torch.tensor(testData)).detach().numpy(), testData).mean())"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.0002584081\n",
            "0.00025840814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJ9YRwUflttR"
      },
      "source": [
        "# representative dataset generator needed for post training quantization\n",
        "def representative_dataset():\n",
        "    for d in testData:\n",
        "      yield [d.astype(np.float32)]"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOjjQzKpxKMB"
      },
      "source": [
        "# here the quantization happens\n",
        "TFLITE_QUANT_FILE_PATH = path + 'models/model_quant.tflite'\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8  # or tf.uint8\n",
        "converter.inference_output_type = tf.int8  # or tf.uint8\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "with open(TFLITE_QUANT_FILE_PATH, 'wb') as f:\n",
        "  f.write(tflite_quant_model)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OK7ZateixKmy",
        "outputId": "867ac989-4222-4670-9255-627bf088041d"
      },
      "source": [
        "# Load the TFLite model in TFLite Interpreter\n",
        "interpreter_quant = tf.lite.Interpreter(TFLITE_QUANT_FILE_PATH)\n",
        "interpreter_quant.allocate_tensors()\n",
        "\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter_quant.get_input_details()\n",
        "output_details = interpreter_quant.get_output_details()\n",
        "\n",
        "# Test the model on random input data.\n",
        "input_shape = input_details[0]['shape']\n",
        "input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n",
        "\n",
        "# Check if the input type is quantized, then rescale input data to uint8\n",
        "if input_details[0]['dtype'] == np.int8:\n",
        "  input_scale, input_zero_point = input_details[0][\"quantization\"]\n",
        "  test = testData[0] / input_scale + input_zero_point\n",
        "\n",
        "test = np.expand_dims(test, axis=0).astype(input_details[0][\"dtype\"])\n",
        "\n",
        "interpreter_quant.set_tensor(input_details[0]['index'], test.astype(np.int8))\n",
        "\n",
        "interpreter_quant.invoke()\n",
        "\n",
        "# The function `get_tensor()` returns a copy of the tensor data.\n",
        "# Use `tensor()` in order to get a pointer to the tensor.\n",
        "output_data = interpreter_quant.get_tensor(output_details[0]['index'])\n",
        "\n",
        "# Convert back to normal precision\n",
        "if input_details[0]['dtype'] == np.int8:\n",
        "  output_scale, output_zero_point = output_details[0][\"quantization\"]\n",
        "  output_data = (output_data - input_zero_point) * input_scale\n",
        "\n",
        "print(output_data)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.06623438  0.08017846  0.0871505  ... -0.17778702 -0.19870314\n",
            "  -0.24750742]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUn8WK1txKyI",
        "outputId": "fe5aaec3-ad5c-49e8-e675-c121f8063e76"
      },
      "source": [
        "print(model(torch.tensor(testData[0]))[:6].detach().numpy())\n",
        "print(testData[0][:6])\n",
        "print(output_data[0][:6])"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.06629637 0.07470772 0.07666905 0.07890725 0.09392202 0.05697285]\n",
            "[0.07025969 0.07882751 0.06280982 0.06567997 0.06773302 0.066687  ]\n",
            "[0.06623438 0.08017846 0.0871505  0.05926234 0.12898274 0.03137418]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SWHqPEsxK9W",
        "outputId": "8c5a955f-ac8f-4a4f-9bee-f3499c03352e"
      },
      "source": [
        "input_details"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'dtype': numpy.int8,\n",
              "  'index': 0,\n",
              "  'name': 'serving_default_input_0:0',\n",
              "  'quantization': (0.0034860200248658657, -128),\n",
              "  'quantization_parameters': {'quantized_dimension': 0,\n",
              "   'scales': array([0.00348602], dtype=float32),\n",
              "   'zero_points': array([-128], dtype=int32)},\n",
              "  'shape': array([   1, 3072], dtype=int32),\n",
              "  'shape_signature': array([  -1, 3072], dtype=int32),\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hExFY6VpDymw",
        "outputId": "393e2b9a-4f42-4833-f4f4-ca7b62c1283c"
      },
      "source": [
        "output_details"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'dtype': numpy.int8,\n",
              "  'index': 8,\n",
              "  'name': 'StatefulPartitionedCall:0',\n",
              "  'quantization': (0.003861609846353531, -125),\n",
              "  'quantization_parameters': {'quantized_dimension': 0,\n",
              "   'scales': array([0.00386161], dtype=float32),\n",
              "   'zero_points': array([-125], dtype=int32)},\n",
              "  'shape': array([   1, 3072], dtype=int32),\n",
              "  'shape_signature': array([  -1, 3072], dtype=int32),\n",
              "  'sparsity_parameters': {}}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rIcIMv1yRMS3",
        "outputId": "1342ecad-3073-42ee-8a1e-c2f5451b7561"
      },
      "source": [
        "!pip install onnxruntime"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.7/dist-packages (1.8.1)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (3.17.3)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.19.5)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.7/dist-packages (from onnxruntime) (1.12)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ac_3E1YRMGx",
        "outputId": "98b427b2-d4d9-4120-fe4e-62062e732412"
      },
      "source": [
        "# testing the validity of the onnx model file\n",
        "import onnxruntime\n",
        "\n",
        "ort_session = onnxruntime.InferenceSession(path + 'models/model.onnx')\n",
        "\n",
        "def to_numpy(tensor):\n",
        "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()\n",
        "\n",
        "# compute ONNX Runtime output prediction\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: to_numpy(x)}\n",
        "ort_outs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "torch_out = model(x)\n",
        "\n",
        "# compare ONNX Runtime and PyTorch results\n",
        "np.testing.assert_allclose(to_numpy(torch_out), ort_outs[0], rtol=1e-03, atol=1e-05)\n",
        "\n",
        "print(\"Exported model has been tested with ONNXRuntime, and the result looks good!\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Exported model has been tested with ONNXRuntime, and the result looks good!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkeO4oKSRMeO",
        "outputId": "89b397a0-049f-40a9-b079-d105192eae9a"
      },
      "source": [
        "torch_out"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.4425e-02,  3.9142e-01, -7.7355e-03,  ...,  3.7237e+01,\n",
              "         6.7363e+01,  9.6777e+01], grad_fn=<LeakyReluBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39ZUHDmQTMVR",
        "outputId": "41b2a187-c0e7-4be7-d4c2-e96e1d3b4ce4"
      },
      "source": [
        "ort_outs[0]"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.4424529e-02,  3.9141583e-01, -7.7355183e-03, ...,\n",
              "        3.7236710e+01,  6.7362564e+01,  9.6776894e+01], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeg9b9TipI-7"
      },
      "source": [
        "with open('/content/gdrive/My Drive/data/autoencoder/test.npy', 'wb') as f:\n",
        "    np.save(f, x)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MMwxWmyGCrA",
        "outputId": "674709d6-3727-4d54-fdd6-06e963b78e41"
      },
      "source": [
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "\n",
        "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "\n",
        "!sudo apt-get update\n",
        "\n",
        "!sudo apt-get install edgetpu-compiler"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2537  100  2537    0     0  72485      0 --:--:-- --:--:-- --:--:-- 74617\n",
            "OK\n",
            "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
            "Hit:1 http://security.ubuntu.com/ubuntu bionic-security InRelease\n",
            "Hit:2 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Hit:12 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:13 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:15 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "edgetpu-compiler is already the newest version (16.0).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 90 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEseMaJFGO4R",
        "outputId": "d93b1c7e-af7a-43f8-e5f1-d79e27e26a8d"
      },
      "source": [
        "# convert tf lite to edge tpu model (change the paths accordingly). Here it should be paid attention which operation are running on the TPU and CPU\n",
        "!edgetpu_compiler \"/content/gdrive/My Drive/data/autoencoder/models/model_quant.tflite\" -o \"/content/gdrive/My Drive/data/autoencoder/models\" "
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Edge TPU Compiler version 16.0.384591198\n",
            "Started a compilation timeout timer of 180 seconds.\n",
            "\n",
            "Model compiled successfully in 216 ms.\n",
            "\n",
            "Input model: /content/gdrive/My Drive/data/autoencoder/models/model_quant.tflite\n",
            "Input size: 1.51MiB\n",
            "Output model: /content/gdrive/My Drive/data/autoencoder/models/model_quant_edgetpu.tflite\n",
            "Output size: 1.55MiB\n",
            "On-chip memory used for caching model parameters: 769.00KiB\n",
            "On-chip memory remaining for caching model parameters: 6.99MiB\n",
            "Off-chip memory used for streaming uncached model parameters: 0.00B\n",
            "Number of Edge TPU subgraphs: 1\n",
            "Total number of operations: 4\n",
            "Operation log: /content/gdrive/My Drive/data/autoencoder/models/model_quant_edgetpu.log\n",
            "\n",
            "Model successfully compiled but not all operations are supported by the Edge TPU. A percentage of the model will instead run on the CPU, which is slower. If possible, consider updating your model to use only operations supported by the Edge TPU. For details, visit g.co/coral/model-reqs.\n",
            "Number of operations that will run on Edge TPU: 1\n",
            "Number of operations that will run on CPU: 3\n",
            "See the operation log file for individual operation details.\n",
            "Compilation child process completed within timeout period.\n",
            "Compilation succeeded! \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiP2TYQ1zxZf"
      },
      "source": [
        "And that's it. We generated from a pytorch model: ONNX model, Keras model, TF lite model, TF lite quantized model and a TF lite model for the edge TPU."
      ]
    }
  ]
}